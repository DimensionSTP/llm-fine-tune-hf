#!/bin/bash
#SBATCH --job-name=train_job
#SBATCH --partition=8gpu
#SBATCH --gres=gpu:8
#SBATCH --nodelist=gpu-8-003
#SBATCH --output=logs/train_output.log
#SBATCH --error=logs/train_error.log

cd ~/llm-fine-tune-hf

module add compilers/cuda/12.8 compilers/gcc/10.2.0 libraries/nccl/2.21.5
source activate myenv

data_type="conversational"
split_ratio=1e-2
is_strict_split=False
dataset_name="tulu"
dataset_format="parquet"
is_preprocessed=False
strategy="deepspeed"
upload_user="Qwen"
model_type="Qwen3-8B"
revision="main"
left_padding=False
is_enable_thinking=False
is_quantized=False
is_peft=False
r=128
lora_alpha=256
target_modules="all-linear"
lora_dropout=0.0
max_length=2048
max_new_tokens=2048
is_bf16=True
batch_size=2
eval_batch_size=8
gradient_accumulation_steps=64
beta=0.0
epsilon=3e-4
epsilon_high=4e-4
num_generations=8
temperature=0.7
top_p=0.8
top_k=20
log_completions=False
importance_sampling_level=token
loss_type=sapo
use_vllm=True
vllm_mode=colocate
vllm_tensor_parallel_size=8
gpu_memory_utilization=0.7
sapo_temperature_pos=1.0
sapo_temperature_neg=1.05
scale_rewards=group
is_reasoning_model=True
is_answer_tag=True
think_format_weight=0.0
answer_format_weight=1.0
match_weight=2.0
code_execution_weight=0.0
rouge_weight=0.0
equation_weight=0.0
lr=5e-7
weight_decay=1e-1
warmup_ratio=5e-2
epoch=2
step=250
workers_ratio=8
use_all_workers=False

if [ "$strategy" = "deepspeed" ]; then
    deepspeed main.py --config-name=grpo.yaml mode=train \
        data_type=$data_type \
        split_ratio=$split_ratio \
        is_strict_split=$is_strict_split \
        dataset_name=$dataset_name \
        dataset_format=$dataset_format \
        is_preprocessed=$is_preprocessed \
        strategy=$strategy \
        upload_user=$upload_user \
        model_type=$model_type \
        revision=$revision \
        left_padding=$left_padding \
        is_enable_thinking=$is_enable_thinking \
        is_quantized=$is_quantized \
        is_peft=$is_peft \
        peft_config.r=$r \
        peft_config.lora_alpha=$lora_alpha \
        peft_config.target_modules=$target_modules \
        peft_config.lora_dropout=$lora_dropout \
        max_length=$max_length \
        max_new_tokens=$max_new_tokens \
        is_bf16=$is_bf16 \
        batch_size=$batch_size \
        eval_batch_size=$eval_batch_size \
        gradient_accumulation_steps=$gradient_accumulation_steps \
        beta=$beta \
        epsilon=$epsilon \
        epsilon_high=$epsilon_high \
        num_generations=$num_generations \
        generation_config.temperature=$temperature \
        generation_config.top_p=$top_p \
        generation_config.top_k=$top_k \
        log_completions=$log_completions \
        importance_sampling_level=$importance_sampling_level \
        loss_type=$loss_type \
        use_vllm=$use_vllm \
        vllm_mode=$vllm_mode \
        vllm_tensor_parallel_size=$vllm_tensor_parallel_size \
        gpu_memory_utilization=$gpu_memory_utilization \
        sapo_temperature_pos=$sapo_temperature_pos \
        sapo_temperature_neg=$sapo_temperature_neg \
        scale_rewards=$scale_rewards \
        reward.is_reasoning_model=$is_reasoning_model \
        reward.is_answer_tag=$is_answer_tag \
        reward.weight.think_format=$think_format_weight \
        reward.weight.answer_format=$answer_format_weight \
        reward.weight.match=$match_weight \
        reward.weight.code_execution=$code_execution_weight \
        reward.weight.rouge=$rouge_weight \
        reward.weight.equation=$equation_weight \
        lr=$lr \
        weight_decay=$weight_decay \
        warmup_ratio=$warmup_ratio \
        epoch=$epoch \
        step=$step \
        workers_ratio=$workers_ratio \
        use_all_workers=$use_all_workers
else
    python main.py --config-name=grpo.yaml mode=train \
        data_type=$data_type \
        split_ratio=$split_ratio \
        is_strict_split=$is_strict_split \
        dataset_name=$dataset_name \
        dataset_format=$dataset_format \
        is_preprocessed=$is_preprocessed \
        strategy=$strategy \
        upload_user=$upload_user \
        model_type=$model_type \
        revision=$revision \
        left_padding=$left_padding \
        is_enable_thinking=$is_enable_thinking \
        is_quantized=$is_quantized \
        is_peft=$is_peft \
        peft_config.r=$r \
        peft_config.lora_alpha=$lora_alpha \
        peft_config.target_modules=$target_modules \
        peft_config.lora_dropout=$lora_dropout \
        max_length=$max_length \
        max_new_tokens=$max_new_tokens \
        is_bf16=$is_bf16 \
        batch_size=$batch_size \
        eval_batch_size=$eval_batch_size \
        gradient_accumulation_steps=$gradient_accumulation_steps \
        beta=$beta \
        epsilon=$epsilon \
        epsilon_high=$epsilon_high \
        num_generations=$num_generations \
        generation_config.temperature=$temperature \
        generation_config.top_p=$top_p \
        generation_config.top_k=$top_k \
        log_completions=$log_completions \
        importance_sampling_level=$importance_sampling_level \
        loss_type=$loss_type \
        use_vllm=$use_vllm \
        vllm_mode=$vllm_mode \
        vllm_tensor_parallel_size=$vllm_tensor_parallel_size \
        gpu_memory_utilization=$gpu_memory_utilization \
        sapo_temperature_pos=$sapo_temperature_pos \
        sapo_temperature_neg=$sapo_temperature_neg \
        scale_rewards=$scale_rewards \
        reward.is_reasoning_model=$is_reasoning_model \
        reward.is_answer_tag=$is_answer_tag \
        reward.weight.think_format=$think_format_weight \
        reward.weight.answer_format=$answer_format_weight \
        reward.weight.match=$match_weight \
        reward.weight.code_execution=$code_execution_weight \
        reward.weight.rouge=$rouge_weight \
        reward.weight.equation=$equation_weight \
        lr=$lr \
        weight_decay=$weight_decay \
        warmup_ratio=$warmup_ratio \
        epoch=$epoch \
        step=$step \
        workers_ratio=$workers_ratio \
        use_all_workers=$use_all_workers
fi
