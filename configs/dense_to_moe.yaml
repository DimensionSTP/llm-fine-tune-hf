# @package _global_
defaults:
  - hydra: hydra

package_name: llm-fine-tune-hf
project_dir: ${oc.env:PROJECT_DIR}/${package_name}
connected_dir: ${oc.env:CONNECTED_DIR}/${package_name}

task: dense_to_moe

upload_user: Qwen
model_type: Qwen3-0.6B
pretrained_model_name: ${upload_user}/${model_type}
revision: main

runtime:
  dtype: bf16
  device: cuda
  trust_remote_code: false
  safe_serialization: true

moe:
  num_experts: 8
  num_experts_per_tok: 2
  norm_topk_prob: true
  output_router_logits: false
  router_aux_loss_coef: 0.001

router:
  # zeros | xavier_uniform | kaiming_uniform
  init_type: xavier_uniform
  gain: 0.02

moe_info: experts_${moe.num_experts}-tok_${moe.num_experts_per_tok}
output_dir: ${connected_dir}/${task}/${model_type}-${moe_info}

project_name: ${task}-${model_type}-${moe_info}

run_name: ${project_name}
work_dir: ${hydra:runtime.cwd}