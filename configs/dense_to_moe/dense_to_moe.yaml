runtime:
  dtype: bfloat16
  device: cuda
  trust_remote_code: true
  safe_serialization: true
  keep_cwd: true

moe:
  num_experts: 8
  num_experts_per_tok: 2
  norm_topk_prob: true
  output_router_logits: false
  router_aux_loss_coef: 0.001

router:
  # zeros | xavier_uniform | kaiming_uniform
  init_type: zeros
  gain: 0.02

moe_info: experts_${dense_to_moe.moe.num_experts}-tok_${dense_to_moe.moe.num_experts_per_tok}
moe_model_dir: ${connected_dir}/dense_to_moe/${model_type}-${dense_to_moe.moe_info}

merge_mode: one2n

targets:
  merge_attention: true
  attn_projs: [q_proj, k_proj, v_proj, o_proj]
  attn_alpha: 1.0

adapter_prefix: sft
base_adapter_dir: ${connected_dir}/adapters/${model_type}/${dense_to_moe.adapter_prefix}

one2n:
  base_adapter: ${dense_to_moe.base_adapter_dir}/full_split
  alphas: [1.0, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65]

n2n:
  adapter_paths: ["split_8_0", "split_8_1", "split_8_2", "split_8_3", "split_8_4", "split_8_5", "split_8_6", "split_8_7"]
  alphas: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

m2n:
  adapter_paths: ["split_4_0", "split_4_1", "split_4_2", "split_4_3"]
  expert_to_adapter: [0, 1, 2, 3, 0, 1, 2, 3]
  group_alphas: [1.0, 0.90]
  per_expert_alphas: [1.0, 1.0, 1.0, 1.0, 0.90, 0.90, 0.90, 0.90]
  expert_to_group: [0, 0, 0, 0, 1, 1, 1, 1]

merged_moe_model_dir: ${connected_dir}/merge_dense_lora_to_moe/${model_type}-${dense_to_moe.moe_info}/${dense_to_moe.adapter_prefix}/${dense_to_moe.merge_mode}/merge_attention=${dense_to_moe.targets.merge_attention}

verify_merge:
  layers: [0, 10, 53]
  experts: [0, 7]
  projs: [up_proj]
  delta_tol: 5.0e-3
  attn_delta_tol: 8.0e-3

router_only_train: false
router_regex: "(^|\\.)model\\.layers\\.[0-9]+\\.mlp\\.(gate|router)\\.(weight|bias)$"