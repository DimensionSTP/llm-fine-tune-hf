# @package _global_
defaults:
  - dataset: grpo_dataset
  - training_arguments: grpo_config
  - trainer: grpo_trainer
  - reward_manager: reward_manager
  - reward_database: reward_database
  - reward_embedding: reward_embedding
  - deepspeed: ds_config
  - test_dataset: test_dataset
  - hydra: hydra

package_name: llm-fine-tune-hf
project_dir: ${oc.env:PROJECT_DIR}/${package_name}
connected_dir: ${oc.env:CONNECTED_DIR}/${package_name}

modality: text
fine_tune_method: grpo

seed: 2025

data_type: structural

split:
  train: train
  val: val

batch_size: 2
eval_batch_size: 8
workers_ratio: 8
use_all_workers: false

split_ratio: 1e-2
is_strict_split: false

dataset_name: tulu
dataset_format: parquet
is_preprocessed: false
data_column_name: input
target_column_name: response
conversation_column_name: messages
prompt_column_name: prompt
solution_column_name: solution
reward_categories_column_name: reward_categories
role_column_name: role
content_column_name: content
assistant_column_name: assistant
upload_user: Qwen
model_type: Qwen3-8B
pretrained_model_name: ${upload_user}/${model_type}
custom_data_encoder_path: ${connected_dir}/merged/data_encoders/${pretrained_model_name}
revision: main
reference_data_encoder_name: Qwen/Qwen3-8B
left_padding: false
is_enable_thinking: false
merged_model_path: ${connected_dir}/merged/models/${pretrained_model_name}
max_length: 2048

user_name: ${oc.env:USER_NAME}
model_detail: ${model_type}
upload_tag: grpo
model_path: ${user_name}/${model_detail}-${upload_tag}

precision: bf16
attn_implementation: flash_attention_2

is_quantized: false
quantization_config:
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  bnb_4bit_compute_dtype: bfloat16
is_peft: false
peft_config:
  r: 128
  lora_alpha: 256
  target_modules: all-linear
  lora_dropout: 0.0
  bias: none
  task_type: CAUSAL_LM
  inference_mode: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

optim: paged_adamw_32bit
lr: 5e-7
weight_decay: 1e-1

scheduler_type: cosine
warmup_ratio: 5e-2

chat_template_kwargs:
  enable_thinking: ${is_enable_thinking}

beta: 0.0
epsilon: 0.2
epsilon_high: null
num_generations: 8
log_completions: false
importance_sampling_level: token
loss_type: sapo
use_vllm: true
vllm_mode: colocate
vllm_tensor_parallel_size: ${devices}
sapo_temperature_pos: 1.0
sapo_temperature_neg: 1.05
scale_rewards: group

reward:
  is_answer_tag: true
  think_start_token: <think>
  think_end_token: </think>
  answer_start_token: <answer>
  answer_end_token: </answer>
  eos_token: <|im_end|>
  timeout: 2
  rouge_type: l
  equation_target_column_name: target
  equation_numbers_column_name: numbers
  weight:
    think_format: 0.0
    answer_format: 0.0
    match: 2.0
    code_execution: 0.0
    rouge: 0.0
    equation: 0.0
    retrieval_hit: 0.0

devices: ${oc.decode:${oc.env:DEVICES,null}}
strategy: deepspeed
logging_steps: 2
is_bf16: true
gradient_accumulation_steps: 64
max_grad_norm: 1
epoch: 2
step: 250

model_name: CausalLM-GRPO
mode: train

project_name: ${model_name}-${dataset_name}-${mode}
reasoning_info: enable_thinking=${is_enable_thinking}
peft_info: is_quantized=${is_quantized}-is_peft=${is_peft}
peft_detail: r=${peft_config.r}-a=${peft_config.lora_alpha}
length_info: max_prompt_length=${max_length}-max_completion_length=${max_new_tokens}
total_batch_size: bs=${batch_size}x${devices}x${gradient_accumulation_steps}
num_generations_info: n_gen=${num_generations}
save_detail: ${model_detail}-loss_type=${loss_type}-${reasoning_info}-${peft_info}-${peft_detail}-${length_info}-${total_batch_size}-${num_generations_info}
logging_name: ${save_detail}-lr${lr}

output_dir: ${connected_dir}/checkpoints/${model_name}/${dataset_name}/${strategy}/${save_detail}
save_strategy: steps
save_total_limit: -1

resume_training: false
resume_from_checkpoint: null
use_validation: true
eval_strategy: epoch

peft_test:
  adapter_path: ${output_dir}
  adapter_name: adapter

max_new_tokens: 2048
do_sample: true
generation_config:
  temperature: 0.7
  top_p: 0.8
  top_k: 20

gpu_memory_utilization: 0.7

test_data_dir: benchmarks/test
test_output_dir: ${connected_dir}/tests/${model_type}
test_output_name: ${dataset_name}

dus_hidden_layers: 54
wus_hidden_scale: 2
wus_scaling_method: concat
wus_attention_scaling: heads
num_safetensors: 5

korean_model_name: beomi/OPEN-SOLAR-KO-10.7B
reasoning_model_name: Qwen/QwQ-32B
reasoning_tokens:
  - ${reward.think_start_token}
  - ${reward.think_end_token}

run_name: ${project_name}
work_dir: ${hydra:runtime.cwd}
